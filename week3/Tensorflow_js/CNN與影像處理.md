#
```


```

##
```


```



##
```


```



##
```


```



##
```


```



##
```


```



##
```


```



##
```


```



##
```


```



##
```


```



##
```


```



##
```


```



##
```


```



##
```


```



## 即時姿勢偵測
```
使用PoseNet預訓練模型進行即時姿勢偵測
PoseNet預訓練模型可以在瀏覽器使用WebCam網路攝影機進行即時的人體姿勢偵測，支援單人或多人的姿勢偵測。

JavaScript程式使用<script>標籤載入PoseNet預訓練模型：
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet"></script>

只能偵測單人姿勢；多人會誤判
```
```
<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Ch16_6.html</title>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet"></script>
</head>
<body>
<video autoplay muted id="video" width="400" height="400"></video>
<canvas id="output" style="position:absolute;top:0;left:0;"></canvas>
<script>
const color = "aqua";  
const boundingBoxColor = "red";
const lineWidth = 2;

function toTuple({y, x}) {
  return [y, x];
}

function drawPoint(ctx, y, x, r, color) {
  ctx.beginPath();
  ctx.arc(x, y, r, 0, 2 * Math.PI);
  ctx.fillStyle = color;
  ctx.fill();
}   
    
function drawKeypoints(keypoints, minConfidence, ctx, scale=1) {
  for (let i = 0; i < keypoints.length; i++) {
    const keypoint = keypoints[i];
    if (keypoint.score < minConfidence) {
      continue;
    }
    const {y, x} = keypoint.position;
    drawPoint(ctx, y * scale, x * scale, 3, color);
  }
}
   
function drawSegment([ay, ax], [by, bx], color, scale, ctx) {
  ctx.beginPath();
  ctx.moveTo(ax * scale, ay * scale);
  ctx.lineTo(bx * scale, by * scale);
  ctx.lineWidth = lineWidth;
  ctx.strokeStyle = color;
  ctx.stroke();
}

function drawSkeleton(keypoints, minConfidence, ctx, scale=1) {
  const adjacentKeyPoints =
    posenet.getAdjacentKeyPoints(keypoints, minConfidence);

  adjacentKeyPoints.forEach((keypoints) => {
    drawSegment(
      toTuple(keypoints[0].position), toTuple(keypoints[1].position), color,
      scale, ctx);
  });
} 

async function setupWebcam() {
  video = document.getElementById("video");
  const stream = await navigator.mediaDevices.getUserMedia({
    "audio": false,
    "video": { facingMode: "user" },
  });
  video.srcObject = stream;
    
  return new Promise((resolve) => {
    video.onloadedmetadata = () => {
      resolve(video);
    };
  });
}
    
async function detect() {
  let minPoseConfidence = 0.1;
  let minPartConfidence = 0.5;
  const pose = await model.estimateSinglePose(video, {
    flipHorizontal: false
  });
  console.log(pose);
  ctx.clearRect(0, 0, canvas.width, canvas.height);
  if (pose.score >= minPoseConfidence) {
    drawKeypoints(pose.keypoints, minPartConfidence, ctx);
    drawSkeleton(pose.keypoints, minPartConfidence, ctx);
  }
  requestAnimationFrame(detect);
}

async function app() {
  await setupWebcam(); 
  video.play();
  videoWidth = video.videoWidth;
  videoHeight = video.videoHeight;
  video.width = videoWidth;
  video.height = videoHeight;
    
  canvas = document.getElementById("output");
  canvas.width = videoWidth;
  canvas.height = videoHeight;
  ctx = canvas.getContext('2d');
    
  model = await posenet.load({
    architecture: 'MobileNetV1',
    outputStride: 16,
    multiplier: 0.75
  });
  detect();
}
app();
</script>
</body>
</html>
```

```
<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<title>Ch16_6a.html</title>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
<script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/posenet"></script>
</head>
<body>
<video autoplay muted id="video" width="400" height="400"></video>
<canvas id="output" style="position:absolute;top:0;left:0;"></canvas>
<script>
const color = "aqua";  
const boundingBoxColor = "red";
const lineWidth = 2;

function toTuple({y, x}) {
  return [y, x];
}

function drawPoint(ctx, y, x, r, color) {
  ctx.beginPath();
  ctx.arc(x, y, r, 0, 2 * Math.PI);
  ctx.fillStyle = color;
  ctx.fill();
}   
    
function drawKeypoints(keypoints, minConfidence, ctx, scale=1) {
  for (let i = 0; i < keypoints.length; i++) {
    const keypoint = keypoints[i];
    if (keypoint.score < minConfidence) {
      continue;
    }
    const {y, x} = keypoint.position;
    drawPoint(ctx, y * scale, x * scale, 3, color);
  }
}
   
function drawSegment([ay, ax], [by, bx], color, scale, ctx) {
  ctx.beginPath();
  ctx.moveTo(ax * scale, ay * scale);
  ctx.lineTo(bx * scale, by * scale);
  ctx.lineWidth = lineWidth;
  ctx.strokeStyle = color;
  ctx.stroke();
}

function drawSkeleton(keypoints, minConfidence, ctx, scale=1) {
  const adjacentKeyPoints =
    posenet.getAdjacentKeyPoints(keypoints, minConfidence);

  adjacentKeyPoints.forEach((keypoints) => {
    drawSegment(
      toTuple(keypoints[0].position), toTuple(keypoints[1].position), color,
      scale, ctx);
  });
} 

async function setupWebcam() {
  video = document.getElementById("video");
  const stream = await navigator.mediaDevices.getUserMedia({
    "audio": false,
    "video": { facingMode: "user" },
  });
  video.srcObject = stream;
    
  return new Promise((resolve) => {
    video.onloadedmetadata = () => {
      resolve(video);
    };
  });
}
    
async function detect() {
  let minPoseConfidence = 0.15;
  let minPartConfidence = 0.1;
  let maxPoseDetections = 5;
  let nmsRadius = 30.0;

  const all_poses = await model.estimatePoses(video, {
    flipHorizontal: false,
    decodingMethod: "multi-person",
    maxDetections: maxPoseDetections,
    scoreThreshold: minPartConfidence,
    nmsRadius: nmsRadius
  });
  console.log(all_poses);    
  ctx.clearRect(0, 0, canvas.width, canvas.height);
  all_poses.forEach(({score, keypoints}) => {
    if (score >= minPoseConfidence) {
      drawKeypoints(keypoints, minPartConfidence, ctx);
      drawSkeleton(keypoints, minPartConfidence, ctx);
    }
  });
  requestAnimationFrame(detect);
}

async function app() {
  await setupWebcam(); 
  video.play();
  videoWidth = video.videoWidth;
  videoHeight = video.videoHeight;
  video.width = videoWidth;
  video.height = videoHeight;
    
  canvas = document.getElementById("output");
  canvas.width = videoWidth;
  canvas.height = videoHeight;
  ctx = canvas.getContext('2d');
    
  model = await posenet.load({
    architecture: 'MobileNetV1',
    outputStride: 16,
    multiplier: 0.75
  });
  detect();
}
app();
</script>
</body>
</html>

```



